{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone P4 - Creative Extension\n",
    "\n",
    "*Jimmy Paul Martin Wilde, Balz Jakob Gabriel Marty, Romain Alexandre Gros* - Group BJR\n",
    "\n",
    "This notebook documents the creative extension to the paper [Signed networks in social media](https://doi.org/10.1145/1753326.1753532) by J. Leskovec, D. Huttenlocher, and J. Kleinberg.\n",
    "\n",
    "### Table of contents\n",
    "\n",
    "* [Loading data from file](#loadingData)\n",
    "* [Replications](#replications)\n",
    "    * [Basic network properties](#basicNetworkProperties)\n",
    "        * [Nodes](#nodes)\n",
    "        * [Edges](#edges)\n",
    "        * [Fraction of positive and negative edges](#fractions)\n",
    "        * [Triads](#triads)\n",
    "    * [Triad statistics](#triadStatistics)\n",
    "        * [|$T_i$|](#T_i)\n",
    "        * [$p(T_i)$](#p(T_i))\n",
    "        * [$p_0(T_i)$](#p_0(T_i))\n",
    "        * [$s(T_i)$](#s(T_i))\n",
    "    * [Summary Replications](#summaryReplications)\n",
    "* [Analyzing properties of the networks](#properties)\n",
    "* [Preference profile of the user](#SVD-PCA)\n",
    "    * [Matrix generation](#matrixgeneration)\n",
    "    * [SVD](#svd)\n",
    "* [Logistic regression](#logreg)\n",
    "\n",
    "**Note:** The raw data have not been uploaded to github. The first three datasets have been provided by the teaching team of the ADA class. The Reddit data can be downloaded [here](https://snap.stanford.edu/data/soc-RedditHyperlinks.html). The file structure should be the following for the code to work as it is:\n",
    "\n",
    "```\n",
    "ada-2020-project-milestone-p3-p3_bjr\n",
    "│   creative_extension.ipynb\n",
    "│\n",
    "└───images\n",
    "│   │   table1.jpg\n",
    "│   │   table3.jpg\n",
    "│\n",
    "└───rawData\n",
    "    │   soc-sign-epinions.txt\n",
    "    │   soc-sign-Slashdot090221.txt\n",
    "    │   wikiElec.ElecBs3.txt\n",
    "    │   soc-redditHyperlinks-title.tsv\n",
    "    │   soc-redditHyperlinks-body.tsv\n",
    "    \n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory  ./results/  already exists\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "\n",
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "from IPython.display import Image\n",
    "import linecache # to read a single line in textfile\n",
    "\n",
    "import os\n",
    "import os.path\n",
    "\n",
    "# Print progress bars (cosmetics)\n",
    "import time\n",
    "import progressbar\n",
    "\n",
    "# Set path to folder used for storing results of lengthy computations...\n",
    "PATH_OUTPUT_FOLDER = \"./results/\"\n",
    "# ...and creating the folder if it doesn't exist already.\n",
    "try:\n",
    "    os.makedirs(PATH_OUTPUT_FOLDER)    \n",
    "    print(\"Directory \" , PATH_OUTPUT_FOLDER ,  \" created \")\n",
    "except FileExistsError:\n",
    "    print(\"Directory \" , PATH_OUTPUT_FOLDER ,  \" already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data from file <a class=\"anchor\" id=\"loadingData\"></a>\n",
    "\n",
    "The variable PATH_TO_DATA to must be set to the relative path to the folder which contains the three raw data files `soc-sign-epinions.txt`, `soc-sign-Slashdot090221.txt`, `wikiElec.ElecBs3.txt`, `soc-redditHyperlinks-title.tsv` and `soc-redditHyperlinks-body.tsv`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TO_DATA = \"./rawData/\"\n",
    "        \n",
    "# set pandas print options\n",
    "pd.set_option('max_rows', 5) # don't show more than 5 rows\n",
    "pd.options.display.float_format = '{:,.3f}'.format # print floats with one three places"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The datasets from the *Epinions* product review Web site and the blog *Slashdot* are already stored in the right format for the purposes of this replication exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FromNodeId</th>\n",
       "      <th>ToNodeId</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>128552</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841370</th>\n",
       "      <td>131825</td>\n",
       "      <td>131826</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>841371</th>\n",
       "      <td>131827</td>\n",
       "      <td>7714</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>841372 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FromNodeId  ToNodeId  Sign\n",
       "0                0         1    -1\n",
       "1                1    128552    -1\n",
       "...            ...       ...   ...\n",
       "841370      131825    131826     1\n",
       "841371      131827      7714     1\n",
       "\n",
       "[841372 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_epinions = \"soc-sign-epinions.txt\"\n",
    "data_epinions = pd.read_table(PATH_TO_DATA+filename_epinions, names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_epinions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FromNodeId</th>\n",
       "      <th>ToNodeId</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549200</th>\n",
       "      <td>82143</td>\n",
       "      <td>81974</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549201</th>\n",
       "      <td>82143</td>\n",
       "      <td>82136</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>549202 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FromNodeId  ToNodeId  Sign\n",
       "0                0         1     1\n",
       "1                0         2     1\n",
       "...            ...       ...   ...\n",
       "549200       82143     81974     1\n",
       "549201       82143     82136     1\n",
       "\n",
       "[549202 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_slashdot = \"soc-sign-Slashdot090221.txt\"\n",
    "data_slashdot = pd.read_table(PATH_TO_DATA+filename_slashdot, names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_slashdot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset from from the Wikipedia voting network on the other hand needs to be parsed manually since it is not structured like a classical `.csv` file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FromNodeId</th>\n",
       "      <th>ToNodeId</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>25</td>\n",
       "      <td>30</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114038</th>\n",
       "      <td>8243</td>\n",
       "      <td>6307</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>114039</th>\n",
       "      <td>3404</td>\n",
       "      <td>6307</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>107080 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FromNodeId  ToNodeId  Sign\n",
       "0                3        30     1\n",
       "1               25        30    -1\n",
       "...            ...       ...   ...\n",
       "114038        8243      6307    -1\n",
       "114039        3404      6307    -1\n",
       "\n",
       "[107080 rows x 3 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filename_wikipedia = \"wikiElec.ElecBs3.txt\"\n",
    "\n",
    "# Initialize list to store the edges of the network\n",
    "data_wikipedia = []\n",
    "\n",
    "with open(PATH_TO_DATA+filename_wikipedia, \"r\", encoding=\"ANSI\") as wiki_file:\n",
    "    \n",
    "    # Initializing variable to keep track which user's status the vote is about\n",
    "    ToNodeId = None\n",
    "    \n",
    "    # Read file line by line and extract the two nodes and the sign for each link\n",
    "    for line in wiki_file:\n",
    "        if line[0]=='U':\n",
    "            ToNodeId = int(line.split('\\t')[1])\n",
    "        if line[0]=='V':\n",
    "            _,Sign,FromNodeId,_,_=line.split('\\t')\n",
    "            data_wikipedia.append([int(FromNodeId), ToNodeId, int(Sign)])\n",
    "\n",
    "# Convert to DataFrame with the same structure as the other datasets\n",
    "data_wikipedia = pd.DataFrame(data_wikipedia, columns=[\"FromNodeId\", \"ToNodeId\", \"Sign\"])\n",
    "# Remove neutral votes and keep only signed connections\n",
    "data_wikipedia = data_wikipedia[data_wikipedia['Sign']!=0]\n",
    "data_wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LookupError                               Traceback (most recent call last)\n",
    "# <ipython-input-5-f4d81017a0dc> in <module>\n",
    "#       4 data_wikipedia = []\n",
    "#       5 \n",
    "# ----> 6 with open(PATH_TO_DATA+filename_wikipedia, \"r\", encoding=\"ANSI\") as wiki_file:\n",
    "#       7 \n",
    "#       8     # Initializing variable to keep track which user's status the vote is about\n",
    "# \n",
    "# LookupError: unknown encoding: ANSI\n",
    "\n",
    "# j'ai cette erreur quand je run la cellule, je pense que c'est à cause de mon OS\n",
    "\n",
    "### Which character encoding do you use to read the wikipedia file? Maybe it works for windows users as well. Feel free to try."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FromNodeId</th>\n",
       "      <th>ToNodeId</th>\n",
       "      <th>Sign</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>28</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>30</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103745</th>\n",
       "      <td>8273</td>\n",
       "      <td>4940</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>103746</th>\n",
       "      <td>8274</td>\n",
       "      <td>8275</td>\n",
       "      <td>-1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>103747 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FromNodeId  ToNodeId  Sign\n",
       "0                3        28     1\n",
       "1                3        30     1\n",
       "...            ...       ...   ...\n",
       "103745        8273      4940     1\n",
       "103746        8274      8275    -1\n",
       "\n",
       "[103747 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This file was generated after parsing the data manually\n",
    "filename_wikipedia = \"soc-sign-wikipedia.txt\"\n",
    "data_wikipedia = pd.read_table(PATH_TO_DATA+filename_wikipedia)\n",
    "data_wikipedia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reddit dataset comes in two parts. Links from one subreddit to another can originate either in the title or the body of a post. For simplicity, we treat these two cases as equivalent. Any additional information available in the dataset, which goes beyond signed edges between nodes, is not considered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames_reddit = [\"soc-redditHyperlinks-title.tsv\",\"soc-redditHyperlinks-body.tsv\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './rawData/soc-redditHyperlinks-title.tsv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-a4639c53e563>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m data_reddit = pd.concat(   [pd.read_table(PATH_TO_DATA+file, header=0,usecols=[0,1,4],\n\u001b[0m\u001b[0;32m      2\u001b[0m                                           names=[\"FromNodeId\",\"ToNodeId\",\"Sign\"]) \n\u001b[0;32m      3\u001b[0m                                                         for file in filenames_reddit], ignore_index=True)                         \n\u001b[0;32m      4\u001b[0m \u001b[1;31m# convert usernames to unique identifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_usernames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reddit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FromNodeId\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ToNodeId\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-a4639c53e563>\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m data_reddit = pd.concat(   [pd.read_table(PATH_TO_DATA+file, header=0,usecols=[0,1,4],\n\u001b[0m\u001b[0;32m      2\u001b[0m                                           names=[\"FromNodeId\",\"ToNodeId\",\"Sign\"]) \n\u001b[0;32m      3\u001b[0m                                                         for file in filenames_reddit], ignore_index=True)                         \n\u001b[0;32m      4\u001b[0m \u001b[1;31m# convert usernames to unique identifier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mIDs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munique_usernames\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfactorize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_reddit\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"FromNodeId\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"ToNodeId\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_table\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    763\u001b[0m         \u001b[1;31m# default to avoid a ValueError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    764\u001b[0m         \u001b[0msep\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\",\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 765\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mlocals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    766\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    767\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    684\u001b[0m     )\n\u001b[0;32m    685\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 686\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    687\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    688\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    450\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    451\u001b[0m     \u001b[1;31m# Create the parser.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 452\u001b[1;33m     \u001b[0mparser\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfp_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    453\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    454\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    944\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"has_index_names\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    945\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 946\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    947\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    948\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[1;34m(self, engine)\u001b[0m\n\u001b[0;32m   1176\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"c\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"c\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1178\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1179\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"python\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\ada\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m   2006\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"usecols\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2007\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2008\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2009\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2010\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas\\_libs\\parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './rawData/soc-redditHyperlinks-title.tsv'"
     ]
    }
   ],
   "source": [
    "data_reddit = pd.concat(   [pd.read_table(PATH_TO_DATA+file, header=0,usecols=[0,1,4],\n",
    "                                          names=[\"FromNodeId\",\"ToNodeId\",\"Sign\"]) \n",
    "                                                        for file in filenames_reddit], ignore_index=True)                         \n",
    "# convert usernames to unique identifier\n",
    "IDs, unique_usernames = pd.factorize(data_reddit[[\"FromNodeId\",\"ToNodeId\"]].values.flatten())\n",
    "data_reddit[[\"FromNodeId\",\"ToNodeId\"]] = IDs.reshape(-1,2)\n",
    "data_reddit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replications <a class=\"anchor\" id=\"replications\"></a>\n",
    "\n",
    "In this first part, we repeat the analysis that the task of the individual asignments on all four datasets. We do so just to see if the results are also valid on the Reddit data. Please note that we do not always make the same design choices as Leskovec *et al.* since we are not trying to replicate tables from the original paper anymore. We rather treat the the data as a directed graph without multi-edges or self-connections. **This part was adapted from Balz Marty's solution to the individual assignments.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The datasets are stored in a dictionary\n",
    "datasets = {'Epinions' : data_epinions, 'Slashdot' : data_slashdot, \n",
    "            'Wikipedia' : data_wikipedia, 'Reddit' : data_reddit}\n",
    "            \n",
    "for key in datasets.keys():\n",
    "    # remove non-unique edges - keep only the last of a set of multi-edges in the order they appear in the edge list.\n",
    "    lengths = [len(datasets[key])]\n",
    "    datasets[key] = datasets[key].drop_duplicates(subset=['FromNodeId', 'ToNodeId'], keep='last')\n",
    "    lengths.append(len(datasets[key]))\n",
    "    # remove edges of nodes with themselves\n",
    "    datasets[key] = datasets[key][datasets[key]['FromNodeId']!=datasets[key]['ToNodeId']]\n",
    "    lengths.append(len(datasets[key]))\n",
    "    nb_non_unique,nb_self = np.diff(lengths)\n",
    "    print(key,\" :\\n\",\"\\t#multi-edges removed : {}\\n\\t#self_conections removed : {}\\n\".format(nb_non_unique,nb_self))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic network properties <a class=\"anchor\" id=\"basicNetworkProperties\"></a>\n",
    "\n",
    "First, we calculate the basic dataset statistics, which were shown in [Table 1](#table1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/table1.jpg', width=400,height=300) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_statistics = pd.DataFrame(index=datasets.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nodes <a class=\"anchor\" id=\"nodes\"></a>\n",
    "\n",
    "The datasets take the form of lists of edges. Given our assumptions from above, the number of nodes in each network is then naturally given by the number of unique nodes connected by these links."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nb_nodes(dataset):\n",
    "    # Calculate the number of unique edges \n",
    "    return len(np.unique(dataset.to_numpy()[:,:2]))\n",
    "\n",
    "dataset_statistics['Nodes'] = [calculate_nb_nodes(dataset) for dataset in datasets.values()]\n",
    "dataset_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Edges <a class=\"anchor\" id=\"edges\"></a>\n",
    "\n",
    "The number of edges corresponds just the number of rows in each dataset. Note that we already removed multi-edges and self-connections."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_nb_edges(dataset):\n",
    "    # Calculate the number of unique links. The weight of the connection is not considered.\n",
    "    return len(np.unique(dataset.to_numpy()[:,:2],axis=0))\n",
    "\n",
    "dataset_statistics['Edges'] = [calculate_nb_edges(dataset) for dataset in datasets.values()]\n",
    "dataset_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Fraction of positive and negative edges <a class=\"anchor\" id=\"fractions\"></a>\n",
    "\n",
    "Now, the fraction of positive and negative *Signs* of the edges is calculated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count positive and negative edges\n",
    "dataset_statistics['+ edges'] = [np.count_nonzero(dataset['Sign'] == +1) for dataset in datasets.values()]\n",
    "dataset_statistics['- edges'] = [np.count_nonzero(dataset['Sign'] == -1) for dataset in datasets.values()]\n",
    "# divide total number of edges\n",
    "dataset_statistics[['+ edges','- edges']] = \\\n",
    "                        dataset_statistics[['+ edges','- edges']].div(dataset_statistics.Edges, axis='index')\n",
    "dataset_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Triads <a class=\"anchor\" id=\"triads\"></a>\n",
    "\n",
    "Counting the number of triads is somewhat more challenging. The function bellow does so by first calculating the list of neighbors for each node. Then it iterates through all nodes. For each of them it looks up its neighbors and the neighbors of each neighbor. Then it counts the number of nodes that are both neighbor and neighbor's neighbor to the original node - another definition of a triad. This yields *six* times the numbers of triads. For each node, the number of triads it participates in is counted *twice* since being a neighbor is a bidirectional connection. And the same triad is counted once for all *three* nodes that participate in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nb_triads(dataset):\n",
    "    \"\"\"This function calculates the number of triads in a dataset\"\"\"\n",
    "    \n",
    "    # extracting edges from dataset\n",
    "    edges=dataset.to_numpy()[:,:2]\n",
    "    # remove non-unique edges - see discussion on edge count\n",
    "    edges=np.unique(edges,axis=0)\n",
    "    # remove edges of nodes with themselves\n",
    "    edges=edges[edges[:,0]!=edges[:,1]]\n",
    "    \n",
    "    # listing neighbors for each node\n",
    "    neighbors = [[] for i in range(np.max(edges)+1)]\n",
    "    for [u,v] in edges:\n",
    "        neighbors[u]+=[v]\n",
    "        neighbors[v]+=[u]\n",
    "    \n",
    "    # iterating through nodes and counting nodes that are both neighbor and neighbor's neighbor to original node\n",
    "    triads = 0    \n",
    "    nodes = np.unique(edges.flatten())\n",
    "    for node in nodes:\n",
    "        neighbors_node = neighbors[node]\n",
    "        neighbors_neighbours = np.concatenate([neighbors[neighbor] for neighbor in neighbors_node])\n",
    "        triads+=sum([np.count_nonzero(neighbors_neighbours==neighbor) for neighbor in neighbors_node])\n",
    "    # correcting for overcounting\n",
    "    assert triads%6 == 0\n",
    "    return triads//6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this is an efficient way of calculating the number of triads, it still takes a few minutes to do so. Ideally, the result is therefore stored to file. If such an output file exists, the result is read from there. Otherwise, the triads are counted and the result gets stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRIAD_COUNT = PATH_OUTPUT_FOLDER + \"triad_count.npy\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### This cell is commented, since it takes a few minutes to run it. If uncommented, the triad count is performed. ###\n",
    "# Calculating the number of triads for each network and saving the result to file.\n",
    "np.save(PATH_TRIAD_COUNT,[int(get_nb_triads(dataset)) for dataset in datasets.values()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output file exists, the result is read from there.\n",
    "if os.path.isfile(PATH_TRIAD_COUNT): \n",
    "    dataset_statistics['Triads'] = np.load(PATH_TRIAD_COUNT)\n",
    "else:\n",
    "    dataset_statistics['Triads'] = [int(get_nb_triads(dataset)) for dataset in datasets.values()]\n",
    "    np.save(PATH_TRIAD_COUNT,dataset_statistics['Triads'].to_numpy())\n",
    "dataset_statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reddit dataset differs from the others in that a greater portion of its edges are positive. What might sound surprising at first glance is probably a consequence of attributing a plus edge to positve and [*neutral*](https://snap.stanford.edu/data/soc-RedditHyperlinks.html) results of sentiment analysis. Note also that the Reddit graph contains more triad despite having fewer edges than the Slashdot graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Triad statistics <a class=\"anchor\" id=\"triadStatistics\"></a>\n",
    "\n",
    "In the following, the analysis to generate [Table 3](#table3) as in milestone P4 is performed on all datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='table1'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('./images/table3.jpg', width=400,height=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### |$T_i$| <a class=\"anchor\" id=\"T_i\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For replicating *Table 1*, I already wrote a function, which counts the number of triads in a given network. Here, I need to keep track of the signs of the edges that make up those triads as well. The function bellow is an adapted version of the previous triad count function that does so.\n",
    "\n",
    "First, it again generates the list of neighbors for each node. In addition, a list of the signs of the corresponding edges is created (-1s are stored as 0s to facilitate keeping track of the number of positive edges in a triad). The function then iterates through all nodes. For each of them it looks up its neighbors and the neighbors of each neighbor. It also counts the number of positive edges out of two edges for each triplet *node-neighbor-neighbor's neighbor*. Finally, for all nodes that are both neighbor and neighbor's neighbor to the original node, it computes the number of positive edges in this corresponding triad. This yields again *six* times the count for each triad type. For each node, the number of triads it participates in is counted *twice* since being a neighbor is a bidirectional connection. And the same triad is counted once for all *three* nodes that participate in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_nb_triads_by_type(dataset):\n",
    "    \"\"\"This function calculates the number of triads in a dataset\"\"\"\n",
    "    \n",
    "    # remove non-unique edges - see discussion on edge count and convert to numpy\n",
    "    edges = dataset.drop_duplicates(subset=['FromNodeId', 'ToNodeId'], keep='last').to_numpy()\n",
    "    # remove edges of nodes with themselves\n",
    "    edges=edges[edges[:,0]!=edges[:,1]]\n",
    "\n",
    "    # listing neighbors for each node and the signs of the corresonding connections\n",
    "    neighbors = [[] for i in range(np.max(edges)+1)]\n",
    "    signs = [[] for i in range(np.max(edges)+1)]\n",
    "\n",
    "    for [u,v,s] in edges:\n",
    "        neighbors[u]+=[v]\n",
    "        neighbors[v]+=[u]\n",
    "        signs[u]+=[max(0,s)]\n",
    "        signs[v]+=[max(0,s)]\n",
    "    \n",
    "    \n",
    "    # iterating through nodes and for each node that is both neighbor and neighbor's neighbor to original node \n",
    "    # counting incrementing the count of the corresponding triad type\n",
    "    triad_counter = Counter() \n",
    "    nodes = np.unique(edges[:,:2].flatten())\n",
    "    \n",
    "    with progressbar.ProgressBar(max_value=nodes.shape[0]) as bar:\n",
    "        for node in nodes:\n",
    "            neighbors_node, signs_node = neighbors[node],signs[node]\n",
    "            neighbors_neighbours = np.concatenate([neighbors[neighbor] for neighbor in neighbors_node])\n",
    "            neighbors_signs = np.concatenate([np.array(signs[neighbor])+sign \\\n",
    "                                              for (neighbor,sign) in zip(neighbors_node,signs_node)])\n",
    "            for (neighbor,sign) in zip(neighbors_node,signs_node):\n",
    "                triad_counter+=Counter(neighbors_signs[neighbors_neighbours==neighbor]+sign)\n",
    "        # converting to dataframe and correcting for overcounting\n",
    "        triad_counter = pd.DataFrame.from_dict(triad_counter, orient='index',columns=['|T_i|'])//6\n",
    "        triad_counter.index.name = '#+edges'\n",
    "    return triad_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_TRIAD_COUNT_BY_TYPE = PATH_OUTPUT_FOLDER + \"triad_count_by_type_\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## The cell is commented, as it takes a fey minutes to run it. If uncommented, the triad count is performed. ##\n",
    "# Calculating the number of triads for each network and saving the result to file.\n",
    "for key, dataset in datasets.items():\n",
    "    get_nb_triads_by_type(dataset).to_pickle(PATH_TRIAD_COUNT_BY_TYPE+key+\".pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output file exists, the result is read from there.\n",
    "triad_statistics = dict()\n",
    "for key, dataset in datasets.items():\n",
    "    if os.path.isfile(PATH_TRIAD_COUNT_BY_TYPE+key+\".pkl\"):\n",
    "        triad_statistics[key] = pd.read_pickle(PATH_TRIAD_COUNT_BY_TYPE+key+\".pkl\")\n",
    "    else:\n",
    "        triad_statistics[key] = get_nb_triads_by_type(dataset)\n",
    "        triad_statistics[key].to_pickle(PATH_TRIAD_COUNT_BY_TYPE+key+\".pkl\")\n",
    "    # print triad count by type\n",
    "    print(key+\" :\\n\", triad_statistics[key], end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a DataFrame that stores features of datasets such as the number of triads per dataset. A quick sanity check shows that we get the same counts as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_features = pd.DataFrame([sum(table['|T_i|']) for table in triad_statistics.values()],index=datasets.keys(),\n",
    "                               columns=['#Triads'])\n",
    "# Check that I get the same total number of triads as before\n",
    "pd.testing.assert_series_equal(dataset_statistics['Triads'],datasets_features['#Triads'],\n",
    "                               check_dtype=False,check_names=False)\n",
    "datasets_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p(T_i)$ <a class=\"anchor\" id=\"p(T_i)\"></a>\n",
    "\n",
    "The next column contains the fraction of triads of type $T_i$. It is a simple division of the number of triads for each type by the total number of triads in a given network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, table),nb_triads in zip(triad_statistics.items(),datasets_features['#Triads']):\n",
    "    table['p(T_i)'] = table['|T_i|']/nb_triads\n",
    "    print(key+\" :\\n\",table,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $p_0(T_i)$ <a class=\"anchor\" id=\"p_0(T_i)\"></a>\n",
    "\n",
    "For the calculation of the *a priori probability of $T_i$*, we need to know the fraction of of positive edges in the network $p$. This was already calculated above and can be recycled here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frac_pos_edges(dataset):\n",
    "    # Calculate the number of unique links. The weight of the connection is not considered.\n",
    "    unique_edges = dataset.drop_duplicates(subset=['FromNodeId', 'ToNodeId'],keep='last')\n",
    "    return np.count_nonzero(unique_edges['Sign'] == +1)/len(unique_edges)\n",
    "\n",
    "datasets_features['p'] = [calculate_frac_pos_edges(dataset) for dataset in datasets.values()]\n",
    "datasets_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The *a priori* probability of $T_i$ based on sign distribution, $p_0(T_i)$, is given by a binomial distribution:\n",
    "$$p_0(T_i)   = {n \\choose k} p^k (1-p)^{ n-k}$$\n",
    ", where $n=3$ is the number of edges per triad, $k$ is the number of positive edges in the given triad type, and $p$ is the fraction of positive edges in the entire network, which is asumed to be the *a priori* probability that an edge is positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import binom\n",
    "for (key, table),p in zip(triad_statistics.items(),datasets_features['p']):\n",
    "    table['p0(T_i)'] = binom.pmf(table.index.get_level_values('#+edges'), 3, p)\n",
    "    print(key+\" :\\n\",table,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $s(T_i)$ <a class=\"anchor\" id=\"s(T_i)\"></a>\n",
    "\n",
    "The surprise, $s(T_i)$, of finding a number, $|T_i|$, of a given triad type, $T_i$, is given by the following formula:\n",
    "$$s(T_i)=\\frac{|T_i|-E[|T_i|]}{\\sqrt{\\Delta p_0(T_i)(1-p_0(T_i))}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (key, table),nb_triads in zip(triad_statistics.items(),datasets_features['#Triads']):\n",
    "    table['s(T_i)']=(table['|T_i|']-table['p0(T_i)']*nb_triads)\\\n",
    "                    /np.sqrt(nb_triads*table['p0(T_i)']*(1-table['p0(T_i)']))\n",
    "    print(key+\" :\\n\",table,end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary Replications <a class=\"anchor\" id=\"summaryReplications\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tabulate import tabulate\n",
    "\n",
    "def pretty_print(table):\n",
    "    ''' This function generates a pretty print for our results '''\n",
    "    \n",
    "    for key in table:\n",
    "        if key == 'Reddit':\n",
    "            new_index = [3, 1, 2, 0]\n",
    "            table[key] = table[key].reindex(new_index)\n",
    "        table[key] = table[key].rename(index={3: 'T₃', 1: 'T₁', 2: 'T₂', 0: 'T₀'})\n",
    "        table[key] = table[key].rename(columns = {'#+edges ':'', '|T_i|':'|Tᵢ|', 'p(T_i)': 'p(Tᵢ)', 'p0(T_i)': 'p₀(Tᵢ)',\n",
    "                                                  's(T_i)' : 's(Tᵢ)'})\n",
    "\n",
    "    \n",
    "    df_names = ['Epinions', 'Slashdot', 'Wikipedia', 'Reddit']\n",
    "\n",
    "    print('\\033[1m'+'\\t\\tPaper table 3 - reproduced.\\n')\n",
    "    print ('\\033[0m')\n",
    "\n",
    "    for key in table:\n",
    "        print('\\t\\t\\t\\033[1m' + key + '\\033[0m')\n",
    "        print(tabulate(table[key], headers = 'keys', tablefmt=\"fancy_grid\", numalign=\"right\", \n",
    "                       floatfmt=(None,'.0f','.3f','.3f','.1f')))\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print(triad_statistics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Reddit dataset does not follow the same pattern as the other datasets when it comes to triad under- and overrepresentation of certain triad types. Curiously, triads with all-positive edges are underrepresented and all other types are overrepresented. This goes against both structural balance theory and its weaker variant (see [original paper](https://doi.org/10.1145/1753326.1753532) by Leskovec et al. for context). One could argue that a subreddit is not a person and thus not subjected to the same rules of social psychology. But it would have been quite plausible that the principle that \"the friend of my friend is my frient\" applies also to groups. We don't see any particular reason why two groups which are friendly with a third group should not be friendly with one another. Further research is needed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyze properties of the networks <a class=\"anchor\" id=\"properties\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from networkx.convert_matrix import from_pandas_edgelist\n",
    "from networkx.algorithms.approximation import clustering_coefficient\n",
    "from networkx import average_shortest_path_length\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "def bootstrap_CI_mean(data, nbr_draws = 1000):\n",
    "    \"\"\" bootstrap_CI_mean returns a 95% CI of the mean of 'data'\n",
    "    data\n",
    "    nbr_draws: number of draws that should be performed to compute the bootstraped CI\n",
    "    \"\"\" \n",
    "    sample_size = len(data)\n",
    "    samples = data.sample(n = sample_size*nbr_draws, replace = True, random_state=0).values\\\n",
    "              .reshape((sample_size,nbr_draws))\n",
    "    means = np.mean(samples,axis=0)\n",
    "    return np.asarray([np.nanpercentile(means, 2.5),np.nanpercentile(means, 97.5)])\n",
    "\n",
    "# -------------------\n",
    "\n",
    "def compute_properties(datasets):\n",
    "    graphs = {}\n",
    "    for key in datasets:\n",
    "        G = nx.from_pandas_edgelist(datasets[key][['FromNodeId','ToNodeId']], source = 'FromNodeId', \n",
    "                                    target = 'ToNodeId')\n",
    "        graphs[key] = G\n",
    "        \n",
    "    properties = {}\n",
    "    for key in datasets:\n",
    "        properties[key] = {}\n",
    "\n",
    "    for key in graphs:\n",
    "        clustering = nx.clustering(graphs[key])\n",
    "        properties[key]['mean_clustering'] = np.array(list(clustering.values())).mean()\n",
    "        properties[key]['95%CI_clustering'] = bootstrap_CI_mean(pd.Series(list(clustering.values())))\n",
    "        \n",
    "    ccs = []\n",
    "    for key in graphs:\n",
    "        ccs_ = max(nx.connected_components(graphs[key]), key = len)\n",
    "        ccs.append([ccs_, len(ccs_)])\n",
    "    print('The fraction of connected components for each dataset are the following:')\n",
    "    print(np.array(ccs)[:,1]/dataset_statistics['Nodes']*100)\n",
    "    print('\\nSince three of our four graphs arent connected, we will compute the average shortest path length for the bigger connected components each time.\\n')\n",
    "    \n",
    "    sample_nb = 200\n",
    "\n",
    "    for idx, key in enumerate(graphs):\n",
    "        print(key)\n",
    "        random.seed(5)\n",
    "        sample = random.sample(range(0, len(ccs[idx][0])-1),sample_nb)\n",
    "        sources = list(ccs[idx][0])\n",
    "        biggest_graph = graphs[key].subgraph(ccs[idx][0]).copy()\n",
    "\n",
    "        with progressbar.ProgressBar(max_value=sample_nb) as bar:\n",
    "            aspl = []\n",
    "            for idx, samp in enumerate(sample):\n",
    "                aspl.append(np.array(list(nx.shortest_path_length(biggest_graph, source = sources[samp]).values())))\n",
    "                bar.update(idx+1)\n",
    "        properties[key]['mean_aspl'] = np.mean(np.concatenate(aspl, axis=0))\n",
    "        properties[key]['95%CI_aspl'] = bootstrap_CI_mean(pd.Series(np.concatenate( aspl, axis=0)), nbr_draws = 10)\n",
    "\n",
    "    return properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_PROPERTIES = PATH_OUTPUT_FOLDER + \"properties.pickle\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### This cell is commented, since it takes a few minutes to run it. If uncommented, the triad count is performed. ###\n",
    "# Calculating the number of triads for each network and saving the result to file.\n",
    "properties = compute_properties(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if os.path.isfile(PATH_PROPERTIES):\n",
    "    with open(PATH_PROPERTIES, 'rb') as f:\n",
    "        properties = pickle.load(f)\n",
    "else:\n",
    "    properties = compute_properties(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = []\n",
    "aspl = []\n",
    "\n",
    "for key in properties:\n",
    "    clustering.append([key, properties[key]['mean_clustering'], properties[key]['95%CI_clustering']])\n",
    "    aspl.append([key, properties[key]['mean_aspl'], properties[key]['95%CI_aspl']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty_print_property(table, clustering = False):\n",
    "    ''' This function generates a pretty print for our results '''\n",
    "    if clustering:\n",
    "        table = table.rename(columns = {1:'Clustering coefficient', 2:'95% CI'})\n",
    "    else:\n",
    "        table = table.rename(columns = {1:'Average shortest path length', 2:'95% CI'})\n",
    "\n",
    "\n",
    "    print(tabulate(table, headers = 'keys', tablefmt=\"fancy_grid\", numalign=\"right\"))\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering = pd.DataFrame(clustering).set_index(0)\n",
    "clustering.index.names = ['Dataset']\n",
    "aspl = pd.DataFrame(aspl).set_index(0)\n",
    "aspl.index.names = ['Dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_print_property(clustering, clustering = True)\n",
    "pretty_print_property(aspl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO: \n",
    "- maybe have a look at Graph Laplacian\n",
    "- plot degree distribution on log-log plot\n",
    "- small wold properties: small-coefficient $\\sigma = \\frac{\\frac{C}{C_r}}{\\frac{L}{L_r}}$, if $\\sigma>1$ (or equivalently $C>>C_r$ and $L\\approx L_r$), [the network is small-world](https://en.wikipedia.org/wiki/Small-world_network#Properties_of_small-world_networks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In [Signed Netwoks in Social Media](https://doi.org/10.1145/1753326.1753532), Leskovec *et al.* discuss all-positive and all-negative subnetworks. And propose the possowing interpretation:\n",
    "    \n",
    "    [B]oth the all-positive and all-negative networks are less well-connected than expected, in the \n",
    "    sense that their largest connected components are smaller than those of their randomized counterparts. While \n",
    "    this may seem initially counterintuitive, one possible interpretation is as follows. The giant components of \n",
    "    real social networks are believed to consist of densely connected clusters linked by less embedded ties. The \n",
    "    all-positive and all-negative networks in the real (rather than randomized) datasets are each biased toward \n",
    "    one side of this balance: the all-positive networks have dense clusters without the bridging provided by less\n",
    "    embedded ties, while the all-negative networks lack a sufficient abundance of dense clusters to sustain a large\n",
    "    component.\n",
    "\n",
    "In the following, we explore this hypothesis. We perform [spectral clustering](https://en.wikipedia.org/wiki/Spectral_clustering) and calcluate the positive and negative edge sign fraction of inter- and intracluster links. According to the interpretation from above, the intracluster edge signs are expected to be more positive and the intercluster edge signs are expected to be more negative than the [previously](#fractions) determined global value.\n",
    "\n",
    "Note that the networks are not fully connected. Thus, we first identify the largest connected component in each network before performing the clustering analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_characteristics(G):\n",
    "    \"\"\"A function that prints the number of edges and nodes in a given graph.\"\"\"\n",
    "    print(\"\\t#Nodes : {}\\n\\t#Edges : {}\".format(G.number_of_nodes(),G.number_of_edges()))\n",
    "\n",
    "# Isolating the largest connected component in a graph\n",
    "connected_graphs = {}\n",
    "for key in datasets:\n",
    "    print(key,\" :\\n\\tfull graph:\")\n",
    "    # Generate an undirected signed graph from an edgelist\n",
    "    G = nx.from_pandas_edgelist(datasets[key][['FromNodeId','ToNodeId','Sign']], source = 'FromNodeId',\n",
    "                                target = 'ToNodeId', edge_attr = 'Sign')\n",
    "    print_characteristics(G)\n",
    "    # Generate the subgraph from the largest connected component\n",
    "    G = G.subgraph(max(nx.connected_components(G), key = len))\n",
    "    print(\"\\tlargest connected subgraph:\")\n",
    "    print_characteristics(G)\n",
    "    connected_graphs[key] = G"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The complexity of the spectral clustering algorithm is ~ O(n³) in the number of nodes. It is therefore not suitable for the larger networks. Instead of sampling the larger graphs (from Epinions, Slashdot, and Reddit), which might alter the result, we ran it only on the wikipedia dataset. Because the real number of clusters is unknown, we vary this parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "def calculate_intra_inter_cluster_edge_sign_fraction(connected_graph, number_of_clusters):\n",
    "    \"\"\"This function performs spectral clustering of a graph and returns the fraction of positive\n",
    "       and negative edges among links between and within clusters.\"\"\"\n",
    "    \n",
    "    # Initialize DataFrame to store results\n",
    "    edge_stats = pd.DataFrame({('intra','+ edges'):[], ('intra','- edges'):[],\n",
    "                               ('inter','+ edges'):[], ('inter','- edges'):[]})\n",
    "    edge_stats.columns.names = ('Edge Type','Sign')\n",
    "    edge_stats.index.name= \"Number of Clusters\"\n",
    "    \n",
    "    # calculate adjacency matrix\n",
    "    adj_matrix = nx.to_scipy_sparse_matrix(connected_graph)\n",
    "    \n",
    "    for n_clusters in number_of_clusters:\n",
    "        # perform spectral clustering\n",
    "        clustering = SpectralClustering(affinity = 'precomputed', assign_labels = \"discretize\", random_state = 42,\n",
    "                                      n_clusters=n_clusters).fit_predict(adj_matrix)\n",
    "        # Store clusters as dictionary - clusterID : [node1,...]\n",
    "        clusters = {clusterID : [i for i,x in enumerate(clustering) if x==clusterID] for clusterID in set(clustering)}\n",
    "        # Generate intra- and intercluster subgraphs\n",
    "        G_intra = nx.Graph()\n",
    "        G_inter = connected_graph.copy()\n",
    "        # Add/remove intracluster connections\n",
    "        for nodes_cluster in clusters.values():\n",
    "            subgraph_cluster = connected_graph.subgraph(nodes_cluster)\n",
    "            G_intra.update(subgraph_cluster)\n",
    "            for edge in list(subgraph_cluster.edges):\n",
    "                G_inter.remove_edge(*edge)\n",
    "        # Store fraction of positive and negative edges\n",
    "        edge_count_intra = Counter(list(nx.get_edge_attributes(G_intra,\"Sign\").values()))\n",
    "        edge_count_inter = Counter(list(nx.get_edge_attributes(G_inter,\"Sign\").values()))\n",
    "        intra_plus = edge_count_intra[1]/G_intra.size()\n",
    "        intra_minus = edge_count_intra[-1]/G_intra.size()\n",
    "        inter_plus = edge_count_inter[1]/G_inter.size()\n",
    "        inter_minus = edge_count_inter[-1]/G_inter.size()\n",
    "        \"\"\"\n",
    "        print(key,\" :\\n\\tintracluster\\n\\t\\t+ edges : {:0.3f}\\n\\t\\t- edges : {:0.3f}\".format(intra_plus,intra_minus),\n",
    "              \"\\n\\tintercluster\\n\\t\\t+ edges : {:0.3f}\\n\\t\\t- edges : {:0.3f}\".format(inter_plus,inter_minus))\n",
    "        \"\"\"\n",
    "        edge_stats.loc[n_clusters] = [intra_plus,intra_minus,inter_plus,inter_minus]\n",
    "    return edge_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set output path\n",
    "PATH_EDGE_SIGN_FRACTION_INTER_INTRA_CLUSTER = PATH_OUTPUT_FOLDER + \"intra_inter_cluster_edge_signs.pkl\"\n",
    "\n",
    "# Only the graph from wikipedia si small enough for the spectral clustering analysis.\n",
    "connected_graph = connected_graphs[\"Wikipedia\"]\n",
    "# Set the number of clusters to try in which the graph is divided.\n",
    "number_of_clusters = np.arange(10,101,1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## The cell is commented, as it takes a fey minutes to run it. If uncommented, the triad count is performed. ##\n",
    "# Calculating the number of triads for each network and saving the result to file.\n",
    "edge_stats = calculate_intra_inter_cluster_edge_sign_fraction(connected_graph, number_of_clusters)\n",
    "edge_stats.to_pickle(PATH_EDGE_SIGN_FRACTION_INTER_INTRA_CLUSTER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If the output file exists, the result is read from there.\n",
    "if os.path.isfile(PATH_EDGE_SIGN_FRACTION_INTER_INTRA_CLUSTER):\n",
    "    edge_stats = pd.read_pickle(PATH_EDGE_SIGN_FRACTION_INTER_INTRA_CLUSTER)\n",
    "else:\n",
    "    edge_stats = calculate_intra_inter_cluster_edge_sign_fraction(connected_graph, number_of_clusters)\n",
    "    edge_stats.to_pickle(PATH_EDGE_SIGN_FRACTION_INTER_INTRA_CLUSTER)\n",
    "\n",
    "# Show intra- and intercluster positive and negative edge fractions\n",
    "edge_stats.plot()\n",
    "plt.ylabel(\"Fraction of Edges\")\n",
    "plt.title(\"Fraction of intra- and intercluster +/- Edges\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Small world properties"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "from networkx.algorithms.smallworld import sigma"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G_wiki = nx.from_pandas_edgelist(datasets[\"Wikipedia\"][['FromNodeId','ToNodeId']], source = 'FromNodeId', \n",
    "                                    target = 'ToNodeId')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nx.connected_components"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "nx.info(graphs[\"Wikipedia\"])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(nx.info(graphs[\"Wikipedia\"]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "print(nx.info(graphs[\"Wikipedia\"].subgraph(max(nx.connected_components(graphs[\"Wikipedia\"]), key=len))))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    import numpy as np\n",
    "\n",
    "    # Compute the mean clustering coefficient and average shortest path length\n",
    "    # for an equivalent random graph\n",
    "    randMetrics = {\"C\": [], \"L\": []}\n",
    "    for i in range(nrand):\n",
    "        Gr = random_reference(G, niter=niter, seed=seed)\n",
    "        Gl = lattice_reference(G, niter=niter, seed=seed)\n",
    "        randMetrics[\"C\"].append(nx.transitivity(Gl))\n",
    "        randMetrics[\"L\"].append(nx.average_shortest_path_length(Gr))\n",
    "\n",
    "    C = nx.transitivity(G)\n",
    "    L = nx.average_shortest_path_length(G)\n",
    "    Cl = np.mean(randMetrics[\"C\"])\n",
    "    Lr = np.mean(randMetrics[\"L\"])\n",
    "\n",
    "    omega = (Lr / L) - (C / Cl)\n",
    "\n",
    "    return omega"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "for g in nx.connected_components(G_wiki):\n",
    "    print(nx.info(g))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ccs = max(nx.connected_components(G_wiki), key = len)\n",
    "print(ccs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "G_wiki = nx.from_pandas_edgelist(datasets[\"Wikipedia\"][['FromNodeId','ToNodeId']], source = 'FromNodeId', \n",
    "                                    target = 'ToNodeId')\n",
    "omega=nx.algorithms.smallworld.sigma(G_wiki,niter =100, nrand=1, seed=42)\n",
    "omega"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "PATH_OMEGAS = PATH_OUTPUT_FOLDER + \"omegas.npy\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "omegas = [nx.algorithms.smallworld.omega(graph,seed=42) for graph in graphs.values()]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### This cell is commented, since it takes a few minutes to run it. If uncommented, the triad count is performed. ###\n",
    "# Calculating the number of triads for each network and saving the result to file.\n",
    "np.save(PATH_OMEGAS,omegas)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# If the output file exists, the result is read from there.\n",
    "if os.path.isfile(PATH_OMEGAS): \n",
    "    omegas = np.load(PATH_OMEGAS)\n",
    "else:\n",
    "    omegas = [nx.algorithms.smallworld.omega(graph) for graph in graphs.values()]\n",
    "    np.save(PATH_OMEGAS,[nx.algorithms.smallworld.omega(graph) for graph in graphs.values()])\n",
    "omegas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic regression <a class=\"anchor\" id=\"logreg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do: generate features which are expected to have more or less predictive power depending on the different theoretical considerations (e.g. the number of common neighbors, a balance and a status score based on the triads that the edge is involved in, etc.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we want to build a model to predict the sign of a given edge. We consider multiple features including number of common neighbors and features created from the different combinations of directed signed triads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data in case they have been modified\n",
    "data_epinions = pd.read_table(PATH_TO_DATA+filename_epinions, \n",
    "                              names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_slashdot = pd.read_table(PATH_TO_DATA+filename_slashdot, \n",
    "                              names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_wikipedia = pd.read_table(PATH_TO_DATA+filename_wikipedia)\n",
    "data_reddit = pd.concat(   [pd.read_table(PATH_TO_DATA+file, header=0,usecols=[0,1,4],\n",
    "                                          names=[\"FromNodeId\",\"ToNodeId\",\"Sign\"]) \n",
    "                                                        for file in filenames_reddit], ignore_index=True)\n",
    "IDs, unique_usernames = pd.factorize(data_reddit[[\"FromNodeId\",\"ToNodeId\"]].values.flatten())\n",
    "data_reddit[[\"FromNodeId\",\"ToNodeId\"]] = IDs.reshape(-1,2)\n",
    "\n",
    "# The datasets are stored in a dictionary\n",
    "datasets = {'Epinions' : data_epinions, 'Slashdot' : data_slashdot, \n",
    "            'Wikipedia' : data_wikipedia, 'Reddit' : data_reddit}\n",
    "            \n",
    "for key in datasets.keys():\n",
    "    # remove non-unique edges - keep only the last of a set of multi-edges in the order they appear in the edge list.\n",
    "    datasets[key] = datasets[key].drop_duplicates(subset=['FromNodeId', 'ToNodeId'], keep='last')\n",
    "    # remove edges of nodes with themselves\n",
    "    datasets[key] = datasets[key][datasets[key]['FromNodeId']!=datasets[key]['ToNodeId']]\n",
    "    \n",
    "datasets = [datasets[key] for key in datasets.keys()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate feature matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we generate the matrix $A$ for each dataset, with entry $(i,j)$ equals to zero if there isn't an edge from $i$ to $j$ and $A(i,j)$ equal to the weight of the edge $i$->$j$ if it exists. Besides, we generate similar matrix $A_+$ and $A_-$ with respectively only positive weights or negative weights, i.e. $A = A_+ + A_-$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_matrices_pos_neg(datasets):\n",
    "    matrices_pos = []\n",
    "    matrices_neg = []\n",
    "    matrices = []\n",
    "    for idx, data in enumerate(datasets):\n",
    "        dimension = max(max(data['FromNodeId'].values), max(data['ToNodeId'].values))+1\n",
    "        matrix_pos = lil_matrix((dimension, dimension))\n",
    "        matrix_neg = lil_matrix((dimension, dimension))\n",
    "        matrix = lil_matrix((dimension, dimension))\n",
    "        edges = data.to_numpy()\n",
    "        for edge in edges:\n",
    "            \n",
    "            matrix[edge[0],edge[1]] = edge[2]\n",
    "            if (edge[2] == 1):\n",
    "                matrix_pos[edge[0],edge[1]] = edge[2]\n",
    "            else:\n",
    "                matrix_neg[edge[0],edge[1]] = edge[2]\n",
    "                \n",
    "        matrix_pos = csr_matrix(matrix_pos)\n",
    "        matrices_pos.append(matrix_pos)\n",
    "        \n",
    "        matrix_neg = csr_matrix(matrix_neg)\n",
    "        matrices_neg.append(matrix_neg)\n",
    "        \n",
    "        matrix = csr_matrix(matrix)\n",
    "        matrices.append(matrix)\n",
    "    \n",
    "    return matrices, matrices_pos, matrices_neg\n",
    "\n",
    "def compute_X(datasets):\n",
    "    \n",
    "    A, A_pos, A_minus = generate_matrices_pos_neg(datasets)\n",
    "\n",
    "    X = np.empty((0,16))\n",
    "    for i in range(len(A)):\n",
    "        X_i = np.array([])\n",
    "        dim = A_pos[i].shape[0]\n",
    "        Xs = []\n",
    "\n",
    "        # remove non-unique edges - see discussion on edge count and convert to numpy\n",
    "        edges = datasets[i].drop_duplicates(subset=['FromNodeId', 'ToNodeId'], keep='last').to_numpy()\n",
    "        # remove edges of nodes with themselves\n",
    "        edges=edges[edges[:,0]!=edges[:,1]]\n",
    "        edges=edges[:,[0,1]]\n",
    "        \n",
    "        for pair_feature in itertools.product([A_pos[i],A_pos[i].T,A_minus[i],A_minus[i].T], repeat=2):\n",
    "            feature = (pair_feature[0]@pair_feature[1]).tocsr()\n",
    "\n",
    "            ### need to keep only the entries with existing edges\n",
    "            feature = np.array(feature[edges[:,0],edges[:,1]])\n",
    "            feature = feature.T\n",
    "            if X_i.shape[0] == 0:\n",
    "                X_i = feature\n",
    "            else:\n",
    "                X_i = np.hstack((X_i,feature))\n",
    "        X = np.vstack((X,X_i)) # add up each feature matrix of the different datasets\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we generate the feature matrix $X$ representing each possible combination for a directed triad. Given an edge $(i,j)$, if $k$ is the third node forming the triad, since $(i,k)$ can take 2 different values (-1 or +1) and can be directed in both direction, there exists 4 different combinations for the $(i,k)$. Similarly there exists 4 configuration for $(j,k)$. Thus, there are 16 different configurations possible for a triad, given an edge.\n",
    "These configurations for edge $(i,j)$ are nothing else than the entry $(i,j)$ of different matrices resulting from combination of the multiplication of two matrices in {$A_+$, $A_-$, $A_+^T$, $A_-^T$}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = compute_X(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we add features that do not specificaly come from a triad disposition. We add for each edge $(i,j)$ the proportion of positive edges directed from/to $i$/$j$. These 4 features should be useful for prediction of edges that are not contained in a triad. We excluded the edge $i$->$j$ itself from its ratio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_nodes_degree(dataset):\n",
    "    \"\"\"This function computes the number of common out neighbors in a dataset\"\"\"\n",
    "    \n",
    "    # extracting edges from dataset\n",
    "    edges=dataset.to_numpy()\n",
    "    # remove non-unique edges - see discussion on edge count\n",
    "    edges=np.unique(edges,axis=0)\n",
    "    # remove edges of nodes with themselves\n",
    "    edges=edges[edges[:,0]!=edges[:,1]]\n",
    "    \n",
    "    # count max(number out neighbors -1,0) for each node\n",
    "    out_degree = -1*np.ones(np.max(edges)+1)\n",
    "    # count max(number in neighbors -1,0) for each node\n",
    "    in_degree = -1*np.ones(np.max(edges)+1)\n",
    "    # count max(number positive out neighbors -1,0) for each node\n",
    "    positive_out = -1*np.ones(np.max(edges)+1)\n",
    "    # count max(number positive in neighbors -1,0) for each node\n",
    "    positive_in = -1*np.ones(np.max(edges)+1)\n",
    "    \n",
    "    for [u,v,w] in edges:\n",
    "        out_degree[u]+=1\n",
    "        in_degree[v]+=1\n",
    "        \n",
    "        if w == 1:\n",
    "            positive_out[u]+=1\n",
    "            positive_in[v]+=1\n",
    "            \n",
    "    out_degree[out_degree<0] = 0\n",
    "    in_degree[in_degree<0] = 0\n",
    "    positive_out[positive_out<0] = 0\n",
    "    positive_in[positive_in<0] = 0\n",
    "    \n",
    "    ratio_out = np.divide(positive_out, out_degree, out=np.zeros_like(positive_out), where=out_degree!=0)\n",
    "    ratio_in = np.divide(positive_in, in_degree, out=np.zeros_like(positive_in), where=in_degree!=0)\n",
    "        \n",
    "    return ratio_out,ratio_in\n",
    "\n",
    "def more_features_X(datasets):\n",
    "    \n",
    "    X_ratio = np.empty((0,4))\n",
    "    \n",
    "    for idx, data in enumerate(datasets):\n",
    "        ratio_out,ratio_in = get_nodes_degree(data)\n",
    "        \n",
    "        # extracting edges from dataset\n",
    "        edges=data.to_numpy()[:,:2]\n",
    "        # remove non-unique edges - see discussion on edge count\n",
    "        edges=np.unique(edges,axis=0)\n",
    "        # remove edges of nodes with themselves\n",
    "        edges=edges[edges[:,0]!=edges[:,1]]\n",
    "        \n",
    "        # add 4 features: ratio_in, ratio out_ for i and j\n",
    "        new_features = np.array([[ratio_out[u],ratio_in[u],ratio_out[v],ratio_in[v]] for [u,v] in edges])\n",
    "        X_ratio = np.vstack((X_ratio,new_features))\n",
    "        \n",
    "    return X_ratio\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ratio = more_features_X(datasets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.hstack((X,X_ratio))\n",
    "sum_ = 0\n",
    "for dataset in datasets:\n",
    "    sum_ += dataset.shape[0]\n",
    "\n",
    "assert sum_ == X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_ = X.copy()\n",
    "sum_ = 0\n",
    "X_Epinions = X[0:datasets[0].shape[0]]\n",
    "sum_ += datasets[0].shape[0]\n",
    "X_Slashdot = X[sum_:sum_+datasets[1].shape[0]]\n",
    "sum_+=datasets[1].shape[0]\n",
    "X_Wikipedia = X[sum_:sum_+datasets[2].shape[0]]\n",
    "sum_+=datasets[2].shape[0]\n",
    "X_Reddit = X[sum_:sum_+datasets[3].shape[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs = [X_Epinions, X_Slashdot, X_Wikipedia, X_Reddit, all_]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model and prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute associated y, which is the sign of each edge\n",
    "\n",
    "ys = []\n",
    "for i in range(len(datasets)):\n",
    "    # remove non-unique edges - see discussion on edge count and convert to numpy\n",
    "    edges = datasets[i].drop_duplicates(subset=['FromNodeId', 'ToNodeId'], keep='last').to_numpy()\n",
    "    # remove edges of nodes with themselves\n",
    "    edges=edges[edges[:,0]!=edges[:,1]]\n",
    "    signs=edges[:,2]\n",
    "    y = signs\n",
    "    y[y==-1] = 0\n",
    "    ys.append(signs)\n",
    "    \n",
    "ys.append(np.hstack(ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "classifier_penalty = ['l1', 'l2']\n",
    "classifier_C = np.logspace(-6, 2, 20)\n",
    "classifier_solver = ['liblinear', 'lbfgs']\n",
    "parameters = [classifier_penalty, classifier_C, classifier_solver]\n",
    "combinations = list(itertools.product(*parameters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_predict,cross_val_score\n",
    "from sklearn.utils._testing import ignore_warnings \n",
    "from sklearn.exceptions import FitFailedWarning, ConvergenceWarning \n",
    "import itertools\n",
    "\n",
    "def optimization(combinations, Xs, ys):\n",
    "# Evaluate the precison/recall with a cross validation (10 splits) and optimize the parameters using a standard\n",
    "# grid search approach\n",
    "    metrics = []\n",
    "\n",
    "    with ignore_warnings(category=[ConvergenceWarning, FitFailedWarning]):\n",
    "        with progressbar.ProgressBar(max_value=len(combinations)) as bar:\n",
    "            for i, comb in enumerate(combinations):\n",
    "                if comb[0] == 'l1' and comb[2] == 'lbfgs':\n",
    "                    continue\n",
    "\n",
    "                logistic = LogisticRegression(penalty = comb[0], C = comb[1], solver=comb[2])\n",
    "\n",
    "                for idx, (X, y) in enumerate(zip(Xs, ys)):\n",
    "                    try:\n",
    "                        accuracy = cross_val_score(logistic, X, y, cv=10, scoring=\"accuracy\")\n",
    "                        precision = cross_val_score(logistic, X, y, cv=10, scoring=\"precision\")\n",
    "                        recall = cross_val_score(logistic, X, y, cv=10, scoring=\"recall\")\n",
    "                        roc_auc = cross_val_score(logistic, X, y, cv=10, scoring=\"roc_auc\")\n",
    "                        metrics.append([comb, idx, [accuracy, precision, recall, roc_auc]])\n",
    "                    except:\n",
    "                        print(idx)\n",
    "                        print(comb)\n",
    "                        pass\n",
    "\n",
    "                bar.update(i+1)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_better (best, value):\n",
    "    if np.mean(value[2][3]) > best[0]:\n",
    "        best[0] = np.mean(value[2][3])\n",
    "        best[1] = value\n",
    "    return best\n",
    "   \n",
    "def get_best_parameters(metrics_all):\n",
    "    best_epi = [0,[]]\n",
    "    best_sls = [0,[]]\n",
    "    best_wiki = [0,[]]\n",
    "    best_reddit = [0,[]]\n",
    "    best_all = [0,[]]\n",
    "\n",
    "    for metric in metrics_all:\n",
    "        if metric[1]==0:\n",
    "            best_epi = is_better(best_epi, metric)\n",
    "        elif metric[1]==1:\n",
    "            best_sls = is_better(best_sls, metric)\n",
    "        elif metric[1]==2:\n",
    "            best_wiki = is_better(best_wiki, metric)\n",
    "        elif metric[1]==3:\n",
    "            best_reddit = is_better(best_reddit, metric)\n",
    "        elif metric[1]==4:\n",
    "            best_all = is_better(best_all, metric)\n",
    "\n",
    "    bests = [best_epi, best_sls, best_wiki, best_reddit, best_all]\n",
    "    best_metrics = []\n",
    "    for idx, best in enumerate(bests):\n",
    "        if len(best[1])>1:\n",
    "            values = best[1][2]\n",
    "            best_metrics.append([idx, best[1][0], np.mean(values[0]), \n",
    "                                 np.mean(values[1]), np.mean(values[2]), np.mean(values[3])])\n",
    "    return best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH_METRICS = PATH_OUTPUT_FOLDER + \"metrics.pickle\"\n",
    "\n",
    "if os.path.isfile(PATH_METRICS): \n",
    "    with open(PATH_METRICS, 'rb') as handle:\n",
    "        best_metrics = pickle.load(handle)\n",
    "else:\n",
    "    best_metrics = get_best_parameters(optimization(combinations, Xs, ys))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0,\n",
       "  ('l2', 0.00012742749857031334, 'lbfgs'),\n",
       "  0.8990971468696769,\n",
       "  0.8984518644935784,\n",
       "  0.9946006953457249,\n",
       "  0.9129309259889633],\n",
       " [1,\n",
       "  ('l2', 0.00012742749857031334, 'lbfgs'),\n",
       "  0.8001008263766748,\n",
       "  0.7986787499935166,\n",
       "  0.991916673535384,\n",
       "  0.759101782328709],\n",
       " [2,\n",
       "  ('l1', 0.11288378916846883, 'liblinear'),\n",
       "  0.8315730132940186,\n",
       "  0.8438687958693756,\n",
       "  0.9655467907743704,\n",
       "  0.8590932201786655],\n",
       " [3,\n",
       "  ('l2', 2.6366508987303555e-06, 'lbfgs'),\n",
       "  0.9243735283623018,\n",
       "  0.925351755729696,\n",
       "  0.9988158925252977,\n",
       "  0.7139662859125188],\n",
       " [4,\n",
       "  ('l2', 2.6366508987303555e-06, 'lbfgs'),\n",
       "  0.852543392867567,\n",
       "  0.852399318280923,\n",
       "  0.9972406977575792,\n",
       "  0.8217492863107481]]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "83.89968445448808"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "edges_pos_all = (Xs[0].shape[0]*85.3 + Xs[1].shape[0]*77.4 + Xs[2].shape[0]*78.8 + Xs[3].shape[0]*92.5)/(Xs[4].shape[0])\n",
    "edges_pos_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Ideas : Should we train on 3 datasets and look how it fits the 4th one? Should we use dimensionnality reduction here to avoid overfitting?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualization of the \"preference profiles\" of users <a class=\"anchor\" id=\"SVD-PCA\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix generation <a class=\"anchor\" id=\"matrixgeneration\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a first time, we have to build matrices from which we will be able to perform SVD. Each row corresponds to an\n",
    "individual, each column corresponds to the link they receive from other individuals. The 'weight' corresponds to the sum of edges signs between the two individuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data in case they have been modified\n",
    "data_epinions = pd.read_table(PATH_TO_DATA+filename_epinions, \n",
    "                              names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_slashdot = pd.read_table(PATH_TO_DATA+filename_slashdot, \n",
    "                              names=[\"FromNodeId\", \"ToNodeId\", \"Sign\"], comment='#')\n",
    "data_wikipedia = pd.read_table(PATH_TO_DATA+filename_wikipedia)\n",
    "data_reddit = pd.concat(   [pd.read_table(PATH_TO_DATA+file, header=0,usecols=[0,1,4],\n",
    "                                          names=[\"FromNodeId\",\"ToNodeId\",\"Sign\"]) \n",
    "                                                        for file in filenames_reddit], ignore_index=True)\n",
    "IDs, unique_usernames = pd.factorize(data_reddit[[\"FromNodeId\",\"ToNodeId\"]].values.flatten())\n",
    "data_reddit[[\"FromNodeId\",\"ToNodeId\"]] = IDs.reshape(-1,2)\n",
    "datasets = [data_epinions, data_slashdot, data_wikipedia, data_reddit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import lil_matrix, csr_matrix\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# ----------\n",
    "\n",
    "def fill_matrix_for_row(row, matrix):\n",
    "    matrix[row['ToNodeId'], row['FromNodeId']] += 1\n",
    "    return row\n",
    "\n",
    "# ----------\n",
    "\n",
    "def generate_matrix(datasets):\n",
    "    matrices = []\n",
    "    for idx, data in enumerate(datasets):\n",
    "        dimension = max(max(data['FromNodeId'].values), max(data['ToNodeId'].values))+1\n",
    "        matrix = lil_matrix((dimension, dimension))\n",
    "        edges = data.to_numpy()\n",
    "        for edge in edges:\n",
    "            matrix[edge[0],edge[1]] += edge[2]\n",
    "        matrix = csr_matrix(matrix)\n",
    "        matrices.append(matrix)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrices = generate_matrix(datasets)\n",
    "\n",
    "# Sanity checka\n",
    "for i, matrix in enumerate(matrices):\n",
    "    assert matrix.sum() == datasets[i]['Sign'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVD <a class=\"anchor\" id=\"svd\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have generated our (sparse) matrices, we can perform svd to allow data dimensionality reduction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd(matrices):\n",
    "    k = 20\n",
    "    matrix_pca = []\n",
    "    for idx, matrix in enumerate(matrices):\n",
    "        svd = TruncatedSVD(k)\n",
    "        matrix_transformed = svd.fit_transform(matrix)\n",
    "        explained_variances = svd.explained_variance_ratio_\n",
    "        matrix_pca.append([matrix_transformed, explained_variances])\n",
    "    return matrix_pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_pca = compute_svd(matrices)\n",
    "\n",
    "idx = 3\n",
    "plt.scatter(matrix_pca[idx][0][:,0], matrix_pca[idx][0][:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def compute_t_SNE(pca):\n",
    "    matrix_t_sne = []\n",
    "    for idx, matrix in enumerate(matrix_pca):\n",
    "        print(idx)\n",
    "        t_sne = TSNE(n_components=2, n_iter=250).fit_transform(matrix[0][:5000])\n",
    "        matrix_t_sne.append(t_sne)\n",
    "    return matrix_t_sne\n",
    "\n",
    "matrix_t_sne = compute_t_SNE(matrix_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 3\n",
    "plt.scatter(matrix_t_sne[idx][:,0], matrix_t_sne[idx][:,1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
